{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget -nc -O data/robert_frost.txt https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/robert_frost.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_plus_one(d, *args):\n",
    "    if len(args) == 1:\n",
    "        # end of recursion: update counter\n",
    "        try:\n",
    "            d[args[0]] += 1\n",
    "        except KeyError:\n",
    "            d[args[0]] = 1\n",
    "\n",
    "        return d\n",
    "    else:\n",
    "        # dig deeper into the rabit hole\n",
    "        if args[0] not in d:\n",
    "            d[args[0]] = {}\n",
    "\n",
    "        d[args[0]] = add_plus_one(d[args[0]], *args[1:])\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "d = {\"A\": {\"B\": {\"C\": 10}}, \"D\": {\"B\": {\"C\": 1}}}\n",
    "print(d)\n",
    "d = add_plus_one(d, \"A\", \"B\", \"C\")\n",
    "print(d)\n",
    "d = add_plus_one(d, \"A\", \"B\", \"D\")\n",
    "print(d)\n",
    "d = add_plus_one(d, \"D\", \"B\", \"C\")\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = {}\n",
    "first_order = {}\n",
    "second_order = {}\n",
    "\n",
    "n_lines_used = 0\n",
    "\n",
    "with open(\"data/robert_frost.txt\", \"r\") as ifn:\n",
    "    for line in ifn:\n",
    "        # Skip empty lines\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "\n",
    "        # Extract tokens\n",
    "        tokens = [tok.rstrip(\"\\n\").lower() for tok in re.findall(r\"\\w+\", line)]\n",
    "        if len(tokens) < 3:\n",
    "            continue\n",
    "\n",
    "        # Append terminating token to the line\n",
    "        tokens.append(\"<END>\")\n",
    "\n",
    "        # Update initial\n",
    "        initial = add_plus_one(initial, tokens[0])\n",
    "\n",
    "        first_order = add_plus_one(first_order, *tokens[:2])\n",
    "\n",
    "        for words in ngrams(tokens, 3):\n",
    "            second_order = add_plus_one(second_order, *words)\n",
    "\n",
    "        n_lines_used += 1\n",
    "\n",
    "print(n_lines_used)\n",
    "print(initial)\n",
    "print(first_order)\n",
    "print(second_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise\n",
    "p_initial = deepcopy(initial)\n",
    "intitial_total = sum(initial.values())\n",
    "for k in initial:\n",
    "    p_initial[k] /= intitial_total\n",
    "p_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise\n",
    "p_first_order = deepcopy(first_order)\n",
    "for k in first_order:\n",
    "    total = sum(first_order[k].values())\n",
    "    for kk in first_order[k]:\n",
    "        p_first_order[k][kk] /= total\n",
    "p_first_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise\n",
    "p_second_order = deepcopy(second_order)\n",
    "for k in second_order:\n",
    "    for kk in second_order[k]:\n",
    "        total = sum(second_order[k][kk].values())\n",
    "        for kkk in second_order[k][kk]:\n",
    "            p_second_order[k][kk][kkk] /= total\n",
    "p_second_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_word(d):\n",
    "    return np.random.choice(list(d.keys()), p=list(d.values()))\n",
    "\n",
    "\n",
    "def generate():\n",
    "    sentence = []\n",
    "\n",
    "    # initital word\n",
    "    w0 = sample_word(p_initial)\n",
    "    sentence.append(w0)\n",
    "\n",
    "    # second word\n",
    "    w1 = sample_word(p_first_order[w0])\n",
    "    sentence.append(w1)\n",
    "\n",
    "    # rest\n",
    "    while True:\n",
    "        w2 = sample_word(p_second_order[w0][w1])\n",
    "        if w2 == \"<END>\":\n",
    "            break\n",
    "\n",
    "        sentence.append(w2)\n",
    "        w0 = w1\n",
    "        w1 = w2\n",
    "\n",
    "    print(\" \".join(sentence))\n",
    "\n",
    "\n",
    "for _ in range(5):\n",
    "    generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
