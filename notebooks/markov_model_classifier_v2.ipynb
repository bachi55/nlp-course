{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov model classifier\n",
    "\n",
    "- [x] try removal of punktuation (how to achieve this with NLTK or spacy)\n",
    "- [ ] try encoding an explicit ```<END-OF-LINE>``` marker as \"terminal state\"\n",
    "- [ ] try training the word to index encoder by trained specifically for each poet\n",
    "- [ ] try using back-of-words or tfidf to encode and classify the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget -nc -O data/edgar_allan_poe.txt https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/edgar_allan_poe.txt\n",
    "wget -nc -O data/robert_frost.txt https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/robert_frost.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from itertools import pairwise\n",
    "\n",
    "import numpy as np\n",
    "from nltk import word_tokenize as word_tokenize_nltk\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import nltk  # noqa: E501\n",
    "# nltk.download(\"punkt\")  # noqa: E501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lines(fn):\n",
    "    with open(fn, \"r\") as ifn:\n",
    "        lines = ifn.readlines()\n",
    "\n",
    "    # Clean up\n",
    "    lines = [ln.rstrip(\"\\n\") for ln in lines]\n",
    "    lines = [ln for ln in lines if ln != \"\\u2009\"]\n",
    "    lines = [ln for ln in lines if len(ln) > 0]\n",
    "\n",
    "    return lines\n",
    "\n",
    "\n",
    "def get_xy(fn, lab=None):\n",
    "    x = read_lines(fn)\n",
    "    if lab is None:\n",
    "        lab = os.path.basename(fn).split(os.extsep)[0]\n",
    "    y = len(x) * [lab]\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def tokenize_document(doc, version):\n",
    "    if version == \"nltk\":\n",
    "        toks = word_tokenize_nltk(doc.lower())\n",
    "    elif version == \"re\":\n",
    "        toks = [tok.lower() for tok in re.findall(r\"\\w+\", doc)]\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid tokeniser version: {version}\")\n",
    "\n",
    "    assert isinstance(toks, list)\n",
    "\n",
    "    return toks\n",
    "\n",
    "\n",
    "class Tok2IndexMapper(object):\n",
    "    \"\"\"A simple class to map tokens to indices and vice versa.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.missing_token_string = \"<MISSING>\"\n",
    "        pass\n",
    "\n",
    "    def fit(self, x):\n",
    "        self._tok2idx = {}\n",
    "        self._idx2toc = []\n",
    "        idx = 0\n",
    "        for doc in x:\n",
    "            for token in doc:\n",
    "                if token not in self._tok2idx:\n",
    "                    self._tok2idx[token] = idx\n",
    "                    self._idx2toc.append(token)\n",
    "                    assert self._idx2toc[idx] == token\n",
    "                    idx += 1\n",
    "\n",
    "        # Add index for missing tokens\n",
    "        assert self.missing_token_string not in self._tok2idx\n",
    "        self._tok2idx[self.missing_token_string] = idx\n",
    "        self._idx2toc.append(self.missing_token_string)\n",
    "        assert self._idx2toc[idx] == self.missing_token_string\n",
    "\n",
    "        return self\n",
    "\n",
    "    def doc2idx(self, doc):\n",
    "        return [self.tok2idx(tok) for tok in doc]\n",
    "\n",
    "    def tok2idx(self, tok):\n",
    "        try:\n",
    "            idx = self._tok2idx[tok]\n",
    "        except KeyError:\n",
    "            idx = self._tok2idx[self.missing_token_string]\n",
    "\n",
    "        return idx\n",
    "\n",
    "    def idx2doc(self, idc):\n",
    "        return [self.idx2toc(idx) for idx in idc]\n",
    "\n",
    "    def idx2toc(self, idx):\n",
    "        return self._idx2toc[idx]\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"Vocab size: {len(self._tok2idx)}\"\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._tok2idx)\n",
    "\n",
    "\n",
    "def estimate_lh_parameters(x, mapper, verbose=False):\n",
    "\n",
    "    n = len(x)  # number of documents / sequences\n",
    "    m = len(mapper)  # number of states\n",
    "\n",
    "    # Count occurances\n",
    "    ci_first = np.zeros((m,))\n",
    "    ci = np.zeros((m,))\n",
    "    for doc in x:\n",
    "        ci_first[doc[0]] += 1  # first occurance\n",
    "        for i in doc:\n",
    "            ci[i] += 1  # any occurance\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"top-10 words (first)\\t:\", mapper.idx2doc(np.argsort(ci_first)[::-1][:10])\n",
    "        )\n",
    "        print(\"top-10 words (all)\\t:\", mapper.idx2doc(np.argsort(ci)[::-1][:10]))\n",
    "\n",
    "    # Count pairwise occurances\n",
    "    cij = np.zeros((m, m))\n",
    "    for doc in x:\n",
    "        for i, j in pairwise(doc):\n",
    "            cij[i, j] += 1\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"top-10 word pairs\\t:\",\n",
    "            [\n",
    "                f\"{mapper.idx2toc(i)} -> {mapper.idx2toc(j)}\"\n",
    "                for i, j in zip(\n",
    "                    *np.unravel_index(np.argsort(cij.flatten())[::-1][:10], cij.shape)\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    # Intitial state distribution\n",
    "    pi = (ci_first + 1) / (n + m)\n",
    "    assert np.allclose(np.sum(pi), 1)\n",
    "\n",
    "    # State transition matrix\n",
    "    a = cij + 1\n",
    "\n",
    "    # Normalize\n",
    "    a = a / a.sum(axis=1)[:, np.newaxis]\n",
    "    assert np.allclose(np.sum(a, axis=1), 1)\n",
    "\n",
    "    return pi, a\n",
    "\n",
    "\n",
    "def estimate_prior_parameters(y):\n",
    "    pi = np.bincount(y) / len(y)\n",
    "    return pi\n",
    "\n",
    "\n",
    "class Classifier:\n",
    "    def __init__(self, logas, logpis, logpriors):\n",
    "        self.logas = logas\n",
    "        self.logpis = logpis\n",
    "        self.logpriors = logpriors\n",
    "        self.K = len(logpriors)  # number of classes\n",
    "\n",
    "    def _compute_log_likelihood(self, input_, class_):\n",
    "        loga = self.logas[class_]\n",
    "        logpi = self.logpis[class_]\n",
    "\n",
    "        last_idx = None\n",
    "        logprob = 0\n",
    "        for idx in input_:\n",
    "            if last_idx is None:\n",
    "                # it's the first token\n",
    "                logprob += logpi[idx]\n",
    "            else:\n",
    "                logprob += loga[last_idx, idx]\n",
    "\n",
    "            # update last_idx\n",
    "            last_idx = idx\n",
    "\n",
    "        return logprob\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        predictions = np.zeros(len(inputs))\n",
    "        for i, input_ in enumerate(inputs):\n",
    "            posteriors = [\n",
    "                self._compute_log_likelihood(input_, c) + self.logpriors[c]\n",
    "                for c in range(self.K)\n",
    "            ]\n",
    "            pred = np.argmax(posteriors)\n",
    "            predictions[i] = pred\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_poe, y_poe = get_xy(\"data/edgar_allan_poe.txt\", 0)\n",
    "x_frost, y_frost = get_xy(\"data/robert_frost.txt\", 1)\n",
    "\n",
    "print(x_poe[:3])\n",
    "print(y_poe[:3])\n",
    "\n",
    "# Combine poets\n",
    "x = x_poe + x_frost\n",
    "y = y_poe + y_frost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepocess the data\n",
    "\n",
    "1. tokenize\n",
    "2. split to train test\n",
    "3. train the token to index mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepocess_data(x, y, tokeniser_version, separate_mapper):\n",
    "    assert tokeniser_version in [\"nltk\", \"re\"]\n",
    "    assert separate_mapper in [True, False]\n",
    "\n",
    "    # Tokenize the documents\n",
    "    x = [tokenize_document(doc, tokeniser_version) for doc in x]\n",
    "\n",
    "    # Split data into training and test sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y, test_size=0.25, random_state=42\n",
    "    )\n",
    "\n",
    "    # Train the mapper on the training data\n",
    "    labs = sorted(set(y_train))\n",
    "    if separate_mapper:\n",
    "        mappers = [\n",
    "            Tok2IndexMapper().fit(\n",
    "                [doc for doc, labi in zip(x_train, y_train) if labi == lab]\n",
    "            )\n",
    "            for lab in labs\n",
    "        ]\n",
    "    else:\n",
    "        mappers = [Tok2IndexMapper().fit(x_train)] * len(labs)\n",
    "\n",
    "    # Convert the data to indices\n",
    "    x_train = [mappers[lab].doc2idx(doc) for doc, lab in zip(x_train, y_train)]\n",
    "    x_test = [mappers[lab].doc2idx(doc) for doc, lab in zip(x_test, y_test)]\n",
    "\n",
    "    return x_train, x_test, y_train, y_test, mappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evalute the models\n",
    "\n",
    "### 1. Preprocess the data\n",
    "### 2. Estimate the parameters of the Markov model\n",
    "\n",
    "**Likelihood**\n",
    "- Transition matrix $\\mathbf{A}$\n",
    "- Intitial state propabilities $\\pi$\n",
    "\n",
    "**Priors**\n",
    "- Priors $\\mathbf{p}$\n",
    "\n",
    "### 3. Predict and evaluate\n",
    "\n",
    "## Version 1: NLTK tokenizer, single mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "x_train, x_test, y_train, y_test, mappers = prepocess_data(x, y, \"nltk\", False)\n",
    "\n",
    "# Estimate the parameters\n",
    "pi_0, A_0 = estimate_lh_parameters(\n",
    "    [x for x, y in zip(x_train, y_train) if y == 0], mappers[0], verbose=False\n",
    ")\n",
    "pi_1, A_1 = estimate_lh_parameters(\n",
    "    [x for x, y in zip(x_train, y_train) if y == 1], mappers[1], verbose=False\n",
    ")\n",
    "\n",
    "priors = estimate_prior_parameters(y_train)\n",
    "\n",
    "# build a classifier\n",
    "clf = Classifier(\n",
    "    [np.log(A_0), np.log(A_1)], [np.log(pi_0), np.log(pi_1)], np.log(priors).tolist()\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "y_hat_train = clf.predict(x_train)\n",
    "print(f\"Train acc: {np.mean(y_hat_train == y_train)}\")\n",
    "print(f\"Train f1: {f1_score(y_train, y_hat_train)}\")\n",
    "print(confusion_matrix(y_train, y_hat_train))\n",
    "\n",
    "y_hat_test = clf.predict(x_test)\n",
    "print(f\"Test acc: {np.mean(y_hat_test == y_test)}\")\n",
    "print(f\"Test f1: {f1_score(y_test, y_hat_test)}\")\n",
    "print(confusion_matrix(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2: RE tokenizer, single mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "x_train, x_test, y_train, y_test, mappers = prepocess_data(x, y, \"re\", False)\n",
    "\n",
    "# Estimate the parameters\n",
    "pi_0, A_0 = estimate_lh_parameters(\n",
    "    [x for x, y in zip(x_train, y_train) if y == 0], mappers[0], verbose=False\n",
    ")\n",
    "pi_1, A_1 = estimate_lh_parameters(\n",
    "    [x for x, y in zip(x_train, y_train) if y == 1], mappers[1], verbose=False\n",
    ")\n",
    "\n",
    "priors = estimate_prior_parameters(y_train)\n",
    "\n",
    "# build a classifier\n",
    "clf = Classifier(\n",
    "    [np.log(A_0), np.log(A_1)], [np.log(pi_0), np.log(pi_1)], np.log(priors).tolist()\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "y_hat_train = clf.predict(x_train)\n",
    "print(f\"Train acc: {np.mean(y_hat_train == y_train)}\")\n",
    "print(f\"Train f1: {f1_score(y_train, y_hat_train)}\")\n",
    "print(confusion_matrix(y_train, y_hat_train))\n",
    "\n",
    "y_hat_test = clf.predict(x_test)\n",
    "print(f\"Test acc: {np.mean(y_hat_test == y_test)}\")\n",
    "print(f\"Test f1: {f1_score(y_test, y_hat_test)}\")\n",
    "print(confusion_matrix(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 3: NLTK tokenizer, separate mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "x_train, x_test, y_train, y_test, mappers = prepocess_data(x, y, \"nltk\", True)\n",
    "\n",
    "# Estimate the parameters\n",
    "pi_0, A_0 = estimate_lh_parameters(\n",
    "    [x for x, y in zip(x_train, y_train) if y == 0], mappers[0], verbose=False\n",
    ")\n",
    "pi_1, A_1 = estimate_lh_parameters(\n",
    "    [x for x, y in zip(x_train, y_train) if y == 1], mappers[1], verbose=False\n",
    ")\n",
    "print(pi_0.shape, A_0.shape)\n",
    "print(pi_1.shape, A_1.shape)\n",
    "priors = estimate_prior_parameters(y_train)\n",
    "\n",
    "# build a classifier\n",
    "clf = Classifier(\n",
    "    [np.log(A_0), np.log(A_1)], [np.log(pi_0), np.log(pi_1)], np.log(priors).tolist()\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "# TODO: When using separate mappers the indices are not consistent. We need to re-apply\n",
    "# each mapper to the data which should be predicted. This is not implemented yet.\n",
    "y_hat_train = clf.predict(x_train)\n",
    "print(f\"Train acc: {np.mean(y_hat_train == y_train)}\")\n",
    "print(f\"Train f1: {f1_score(y_train, y_hat_train)}\")\n",
    "print(confusion_matrix(y_train, y_hat_train))\n",
    "\n",
    "y_hat_test = clf.predict(x_test)\n",
    "print(f\"Test acc: {np.mean(y_hat_test == y_test)}\")\n",
    "print(f\"Test f1: {f1_score(y_test, y_hat_test)}\")\n",
    "print(confusion_matrix(y_test, y_hat_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
