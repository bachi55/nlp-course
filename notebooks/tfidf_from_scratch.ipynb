{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget -nc -O data/bbc_text_cls.csv https://lazyprogrammer.me/course_files/nlp/bbc_text_cls.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize as nltk_word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.en import English\n",
    "\n",
    "spacy_nlp = English()\n",
    "spacy_word_tokenizer = spacy_nlp.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/bbc_text_cls.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize the document and build Word -> Index mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tokens = nltk_word_tokenize(\"This is a test sentence.\")\n",
    "print(nltk_tokens)\n",
    "spacy_tokens = spacy_word_tokenizer(\"This is a test sentence.\")\n",
    "print(len(spacy_tokens))\n",
    "print([token.text for token in spacy_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2idx_mapping(docs, tokenizer):\n",
    "    w2i = {}\n",
    "    i2w = {}\n",
    "    idx = 0\n",
    "    docs_tokenized = []\n",
    "    for doc in docs:\n",
    "        doc_tokenized = []\n",
    "        for token in tokenizer(doc.lower()):\n",
    "            if hasattr(token, \"text\"):\n",
    "                token = token.text\n",
    "\n",
    "            if token not in w2i:\n",
    "                w2i[token] = idx\n",
    "                i2w[idx] = token\n",
    "                idx += 1\n",
    "\n",
    "            doc_tokenized.append(w2i[token])\n",
    "\n",
    "        docs_tokenized.append(doc_tokenized)\n",
    "\n",
    "    return w2i, i2w, docs_tokenized\n",
    "\n",
    "\n",
    "w2i_nltk, i2w_nltk, docs_tok_nltk = get_word2idx_mapping(df[\"text\"], nltk_word_tokenize)\n",
    "w2i_spacy, i2w_spacy, docs_tok_spacy = get_word2idx_mapping(\n",
    "    df[\"text\"], spacy_word_tokenizer\n",
    ")\n",
    "\n",
    "print(\"Vocabulary size (nltk):\", len(w2i_nltk))\n",
    "print(\"Vocabulary size (spacy):\", len(w2i_spacy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the term-frequency matrix (TF)\n",
    "\n",
    "There are actually many ways to calculate the TF matrix. See [wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Term_frequency). Here, we use the simple cound of the terms as TF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_matrix(docs_tokenized, word2idx):\n",
    "    tf = np.zeros((len(docs_tokenized), len(word2idx)))\n",
    "    for i, doc in enumerate(docs_tokenized):\n",
    "        for token in doc:\n",
    "            tf[i, token] += 1\n",
    "\n",
    "    return tf\n",
    "\n",
    "\n",
    "tf_nltk = get_tf_matrix(docs_tok_nltk, w2i_nltk)\n",
    "tf_spacy = get_tf_matrix(docs_tok_spacy, w2i_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the inverse document-frequency (IDF)\n",
    "\n",
    "Also for the inverse document frequency, the IDF matrix, we can find multiple\n",
    "implementations. See [wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Inverse_document_frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idf(tf):\n",
    "    # Total number of documents\n",
    "    n = tf.shape[0]\n",
    "\n",
    "    # Number of documents containing each word\n",
    "    ni = (tf > 0).sum(axis=0)\n",
    "\n",
    "    # Inverse document frequency (smooth)\n",
    "    idf = np.log(n / (1 + ni)) + 1\n",
    "\n",
    "    # remove singlton dimension\n",
    "    idf = np.array(idf).squeeze()\n",
    "\n",
    "    return idf\n",
    "\n",
    "\n",
    "idf_nltk = get_idf(tf_nltk)\n",
    "idf_spacy = get_idf(tf_spacy)\n",
    "print(idf_nltk.shape)\n",
    "print(idf_spacy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine TF and IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(tf, idf):\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_nltk = get_tfidf(tf_nltk, idf_nltk)\n",
    "tfidf_spacy = get_tfidf(tf_spacy, idf_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use sklearn's implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_tfidf = TfidfVectorizer(lowercase=True, norm=None)\n",
    "tfidf_sklearn = np.array(sk_tfidf.fit_transform(df[\"text\"]).todense())\n",
    "tfidf_sklearn.shape\n",
    "i2w_sklearn = {i: w for i, w in enumerate(sk_tfidf.get_feature_names_out())}  # noqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top5(i, i2w, tfidf):\n",
    "    row = df.iloc[i]\n",
    "    print(\"Label:\", row[\"labels\"])\n",
    "    print(\"Text:\", row[\"text\"].split(\"\\n\", 1)[0])\n",
    "    print(\"Top 5 terms:\")\n",
    "\n",
    "    scores = tfidf[i]\n",
    "    indices = (-scores).argsort()\n",
    "\n",
    "    for j in indices[:5]:\n",
    "        print(i2w[j], scores[j])\n",
    "\n",
    "\n",
    "i = np.random.choice(len(df))\n",
    "get_top5(i, i2w_nltk, tfidf_nltk)\n",
    "print()\n",
    "get_top5(i, i2w_spacy, tfidf_spacy)\n",
    "print()\n",
    "get_top5(i, i2w_sklearn, tfidf_sklearn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
